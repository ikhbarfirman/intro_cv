{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Traditional Computer Vision to Artificial Intelligence\n",
    "\n",
    "\n",
    "To navigate up and down, you can use the up and down arrow keys on your keyboard<br />\n",
    "To execute code in this workbook, select the code block and press **Shift+Enter** <br />\n",
    "To edit the code block, press enter. \n",
    "\n",
    "#### The codes in this workbook are cumulative. (Variables defined continue to be available until the notebook is closed) <br />\n",
    "#### So do start from the top and work your way down to avoid unexpected results!\n",
    "\n",
    "\n",
    "For more help on using Jupyter Notebook, you can click on Help > User Interface Tour in the menu above, <br />\n",
    "or visit https://jupyter-notebook.readthedocs.io/en/stable/ui_components.html\n",
    "\n",
    "Experiment and test out your ideas, for that is one of the fastest ways to learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What if we did not need to manually define rules to solve a classification problem?\n",
    "\n",
    "In the last session, you experimented with various basic image processing techniques, and explored how computers could “see”. You then attempted to make the system recognize you by using objects that you held up in front of the camera. You explored a variety of methods, and many of your creative methods likely involved defining rules or “if-else” logic. For example, rules for what were considered “authorized” colors, position, or a combination of conditions. \n",
    "\n",
    "But what if you did not need to define those rules manually?\n",
    "\n",
    "**Machine Learning** is a subset of Artificial Intelligence that focuses on the ability of machines to learn based on training data. Applied to the field of computer vision, what if we could get the machine to learn what was an “authorized” or “unauthorized” image, instead of having to define rules for the exact color codes?\n",
    "\n",
    "In today's session, we will explore how basic computer vision techniques can be combined with machine learning to solve a variety of challenges.\n",
    "1. First, we will jump right into building a simple model to illustrate machine learning\n",
    "1. Then we will take a step back to see the steps involved in building a classification model\n",
    "1. Next, we use classification models to make inferences and explore the accuracy.\n",
    "1. Along the way, do look out for and take note of the limitations and motivations for the different methods and techniques used.\n",
    "\n",
    "\n",
    "## Classifying a card into 1 out of 3 possible categories\n",
    "\n",
    "Let us take a quick look at the \"access cards\" challenge again. <br />\n",
    "Below there are 3 cards (red, green and black cards), and a background scene when no cards are placed in front of the camera.\n",
    "The top row shows the cards held further away, while the bottom row shows the cards held very close to the web camera.\n",
    "\n",
    "\n",
    "<img src=\"[Dataset] Module 21 images/cards.png\" style=\"float:left;\"/>\n",
    "<div style=\"clear:both;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us scope the problem assuming that cards need to be held close to the web camera for validation, then it could just be a matter of comparing the colors of each card (image) to determine which of the 3 cards it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Feature Extraction - Selecting what feature(s) to use to help us infer\n",
    "\n",
    "For this experiment, we have decided to use color to help us to distinguish the cards. But how will we select our color features? Should we select a particular point (e.g. center of the image), or the average color of the image? Should we use a particular channel of the BGR image, or should we convert it to greyscale or any of the other color spaces? \n",
    "\n",
    "The selection of our features will impact the robustness of your solution, and selecting irrelevant \"features\" would not be useful.\n",
    "\n",
    "For example, if we try to use the camera image size to determine whether or not it was an authorized card, it would NOT be relevant since the camera image size will not change regardless of what card is placed in front of the camera. \n",
    "\n",
    "You can try to experiment with different features. <br />\n",
    "But in the meantime, let us do a quick experiment using the average color as the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Let's read the images into memory\n",
    "red_card = cv2.imread(\"[Dataset] Module 21 images/cardred_close.png\")\n",
    "green_card = cv2.imread(\"[Dataset] Module 21 images/cardgreen_close.png\")\n",
    "black_card = cv2.imread(\"[Dataset] Module 21 images/cardblack_close.png\")\n",
    "background = cv2.imread(\"[Dataset] Module 21 images/cardnone.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing and Feature Extraction**\n",
    "\n",
    "Sometimes, we might need to do preprocessing on our input data to ensure that they are of a consistent format that the model accepts. \n",
    "\n",
    "What are some ways that we can preprocess our data?\n",
    "1. Resizing to a standard size.\n",
    "2. Changing image orientation.\n",
    "3. Converting to a particular color space. \n",
    "\n",
    "In this particular example, our loaded input images are already in a consistent landscape (640x480) format in the default BGR color space. But our simple model will not be using all the pixels of the image as features for prediction. Instead, we will be using the average color as a feature for the model to infer the class that it belongs to. Hence, we will next be defining a method to extract the average color from each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract our feature (average color)\n",
    "def averagecolor(image):\n",
    "    return np.mean(image, axis=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used np.mean since the average color has 3 channels (and not a single numerical value). To understand how np.mean works, you can refer to the documentation at \n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html#numpy.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's explore: what are the extracted features (average color) for our red and green cards?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 27.35627604   4.48305664 154.21746094]\n"
     ]
    }
   ],
   "source": [
    "print (averagecolor(red_card))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119.53976563 133.40338216  61.1089388 ]\n"
     ]
    }
   ],
   "source": [
    "print (averagecolor(green_card))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the value generated are different? In fact, their values are very far from each other. This is good! This mean that average color is a good feature for this simple problem.\n",
    "\n",
    "**Now what if we had chosen to use image size as our feature?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "print (red_card.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "print (green_card.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would be able to tell the red card and the green card apart if you only knew their shape? \n",
    "No! As their shape are identical. \n",
    "\n",
    "How about if you knew their average colors?\n",
    "\n",
    "As we can see above, the average color of the red card and the green card are quite different. But the image size of both cards are exactly the same! Since we want to use the features to tell the cards apart, we will go ahead to use average color to help us to infer the type of cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create variables to input the average color value and the label of each image file. We will use this later for model training. Do you remember how this training is done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('red', array([ 27.35627604,   4.48305664, 154.21746094]))\n",
      "('green', array([119.53976563, 133.40338216,  61.1089388 ]))\n",
      "('black', array([70.36474609, 61.85563477, 67.1775651 ]))\n",
      "('none', array([247.9326888 , 241.13666016, 241.89832357]))\n"
     ]
    }
   ],
   "source": [
    "# Store the features (average color) and corresponding label (red/green/black/none) for classification\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "# loop through the cards and print the average color\n",
    "for (card,label) in zip((red_card,green_card,black_card,background),(\"red\",\"green\",\"black\",\"none\")):\n",
    "    print((label, averagecolor(card)))\n",
    "    trainX.append(averagecolor(card))\n",
    "    trainY.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the previous session how the array representation defaults to the order [Blue, Green, Red]\n",
    "\n",
    "Notice how the red card has a much higher value of red than the rest. For the green card, we see that it has higher values of blue and green, and not just green.\n",
    "\n",
    "trainX now stores the feature vectors (features), and trainY stores the corresponding labels.\n",
    "\n",
    "If you are wondering what is stored inside trainX and what is stored inside trainY, do print out the arrays and see for yourself (comparing against the print outs above) It is helpful that you understand how data is being stored at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 27.35627604,   4.48305664, 154.21746094]), array([119.53976563, 133.40338216,  61.1089388 ]), array([70.36474609, 61.85563477, 67.1775651 ]), array([247.9326888 , 241.13666016, 241.89832357])]\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "print(trainX)\n",
    "print(np.array(trainX).shape)      #Note how the 3 channels are stored in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'green', 'black', 'none']\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "print(trainY)\n",
    "print(np.array(trainY).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take more images, you may find that the average color is not always the same exact value, and it will likely fluctuate due to lighting conditions and camera settings. Hence, training a model usually involves more than just a few images. But we will use just these few images just to illustrate the concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Introducing the K-Nearest Neighbour (kNN) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we hold a new card in front of the camera, we want to determine which of these cards it is most similar to. Instead of defining the exact color codes, we might approach it from the angle of \"**Which of our known existing cards is the new card most similar to?**\"\n",
    "\n",
    "The concept of k-Nearest Neighbours is to search the set of labelled images for k most-similar images to the new image. And based on that labels of those similar images, predict the label for the new image. \n",
    "\n",
    "We will run an experiment below for k=1. That is, to find 1 image with the most similar average color to the new image. And use the label for that image to predict the label for the new image.\n",
    "\n",
    "Let's break down how this is done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we read the new image into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_card = cv2.imread(\"[Dataset] Module 21 images/test/16.png\")\n",
    "new_card_features = averagecolor(new_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the distances between the features (average color) of that new image against the features of the images we know\n",
    "Read about linealg.norm [here](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117.79791641023513, 113.43645699355922, 33.497714831624535, 340.3000785919897]\n"
     ]
    }
   ],
   "source": [
    "calculated_distances = []\n",
    "for card in (trainX):\n",
    "    calculated_distances.append(np.linalg.norm(new_card_features-card))\n",
    "    \n",
    "print (calculated_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And here is the result of the which card it is most similar to:\n",
    "Can you guess just by looking at calculated_distances above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black\n"
     ]
    }
   ],
   "source": [
    "print(trainY[np.argmin(calculated_distances)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do open the images/test subfolder and check the actual colors of the respective images.\n",
    "\n",
    "Note that the distance measure we used was \"np.linalg.norm()\". You can read up more about it at https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html or Search the Internet for \"Euclidean Distance\". In simple terms, you can just take it as a measure of how similar the array values of (new_card_features) and (card) are.\n",
    "\n",
    "Do take some time to also understand what the last line does. Recall what is stored inside trainY in section 1.1.\n",
    "\n",
    "Check what is stored inside calculated_distances. \n",
    "What does np.argmin do? \n",
    "\n",
    "Hint: Lookup the documentation for numpy.argmin if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117.79791641023513, 113.43645699355922, 33.497714831624535, 340.3000785919897]\n"
     ]
    }
   ],
   "source": [
    "print(calculated_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(np.argmin(calculated_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'green', 'black', 'none']\n"
     ]
    }
   ],
   "source": [
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black\n"
     ]
    }
   ],
   "source": [
    "print(trainY[np.argmin(calculated_distances)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try testing another card\n",
    "Remember to check your folder to ensure that the model can indeed predict what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "# First we read the new image into memory\n",
    "new_card = cv2.imread(\"[Dataset] Module 21 images/test/36.png\")\n",
    "new_card_features = averagecolor(new_card)\n",
    "\n",
    "# Calculate the distances between the features (average color) of that new image against the features of the images we know\n",
    "calculated_distances = []\n",
    "for card in (trainX):\n",
    "    calculated_distances.append(np.linalg.norm(new_card_features-card))\n",
    "\n",
    "# And here is the result of the which card it is most similar to:\n",
    "print(trainY[np.argmin(calculated_distances)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about another card?\n",
    "Remember to check your folder to ensure that the model can indeed predict what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green\n"
     ]
    }
   ],
   "source": [
    "# First we read the new image into memory\n",
    "new_card = cv2.imread(\"[Dataset] Module 21 images/test/56.png\")\n",
    "new_card_features = averagecolor(new_card)\n",
    "\n",
    "# Calculate the distances between the features (average color) of that new image against the features of the images we know\n",
    "calculated_distances = []\n",
    "for card in (trainX):\n",
    "    calculated_distances.append(np.linalg.norm(new_card_features-card))\n",
    "\n",
    "# And here is the result of the which card it is most similar to:\n",
    "print(trainY[np.argmin(calculated_distances)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to classify all the test cards\n",
    "\n",
    "Not bad! It seems that our simplistic model has correctly classified the cards so far. \n",
    "\n",
    "Let us try looping over and classifying all the cards in the test subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Ground truth for the test images. Open the folder on your computer to see the images.\n",
    "realtestY = np.array([\"black\",\"black\",\"black\",\"black\",\"black\",\n",
    "                     \"red\",\"red\",\"red\",\"red\",\"red\",\n",
    "                     \"green\",\"green\",\"green\",\"green\",\"green\",\n",
    "                     \"none\",\"none\",\"none\",\"none\",\"none\"])\n",
    "def evaluateaccuracy(filenames,predictedY):\n",
    "    predictedY = np.array(predictedY)\n",
    "    if (np.sum(realtestY!=predictedY)>0):\n",
    "        print (\"Wrong Predictions: (filename, labelled, predicted) \")\n",
    "        print (np.dstack([filenames,realtestY,predictedY]).squeeze()[(realtestY!=predictedY)])\n",
    "    # Calculate those predictions that match (correct), as a percentage of total predictions\n",
    "    return \"Correct :\"+ str(np.sum(realtestY==predictedY)) + \". Wrong: \"+str(np.sum(realtestY!=predictedY)) + \". Correctly Classified: \" + str(np.sum(realtestY==predictedY)*100/len(predictedY))+\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were you surprised that there was no output for the block of code above? That is because we only defined the function to do the accuracy evaluation. To learn more about functions in Python, you can visit [this link](https://www.datacamp.com/community/tutorials/functions-python-tutorial)\n",
    "\n",
    "Let us run the code block below to see the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.png: black\n",
      "17.png: black\n",
      "18.png: black\n",
      "19.png: black\n",
      "20.png: black\n",
      "36.png: red\n",
      "37.png: red\n",
      "38.png: red\n",
      "39.png: red\n",
      "40.png: red\n",
      "56.png: green\n",
      "57.png: green\n",
      "58.png: none\n",
      "59.png: green\n",
      "60.png: green\n",
      "76.png: none\n",
      "77.png: none\n",
      "78.png: none\n",
      "79.png: none\n",
      "80.png: none\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       1.00      1.00      1.00         5\n",
      "       green       1.00      0.80      0.89         5\n",
      "        none       0.83      1.00      0.91         5\n",
      "         red       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.96      0.95      0.95        20\n",
      "weighted avg       0.96      0.95      0.95        20\n",
      "\n",
      "\n",
      "Wrong Predictions: (filename, labelled, predicted) \n",
      "[['58.png' 'green' 'none']]\n",
      "Correct :19. Wrong: 1. Correctly Classified: 95.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"[Dataset] Module 21 images/test/\"\n",
    "predictedY = []\n",
    "filenames = []\n",
    "for filename in os.listdir(path):\n",
    "    img = cv2.imread(path+filename)\n",
    "    img_features = averagecolor(img)\n",
    "    calculated_distances = []\n",
    "    for card in (trainX):\n",
    "        calculated_distances.append(np.linalg.norm(img_features-card))\n",
    "    prediction = trainY[np.argmin(calculated_distances)]\n",
    "    \n",
    "    print (filename + \": \" + prediction) #Print out the inferences\n",
    "    filenames.append(filename)\n",
    "    predictedY.append(prediction)\n",
    "\n",
    "# Evaluate Accuracy (the sklearn package provides a useful report)\n",
    "print ()\n",
    "print(classification_report(realtestY, predictedY))\n",
    "\n",
    "# Evaluate Accuracy (our own custom method to output the filenames of the misclassified entries)\n",
    "print ()\n",
    "print (evaluateaccuracy(filenames,predictedY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does precision and recall mean?**\n",
    "Do you remember we've went through these during the Foundation stage?\n",
    "\n",
    "Remember the concepts of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "For example, if we are evaluating the red class:\n",
    "- If you classify a red image correctly as red, that is a true positive.\n",
    "- If you classify a red image wrongly as black, that is a false negative.\n",
    "- If you classify another non-red image as red, that is a false positive.\n",
    "- If you classify a non-red image correctly as non-red, that is a true negative.\n",
    "\n",
    "Precision is the number of True Positives divided by (True Positives + False Positives) i.e. how many out of that were classified red were actually red.\n",
    "\n",
    "Recall is the number of True Positives divided by (True Positives + False Negatives) i.e. how many red images were correctly classified red when you tried to get all the red images.\n",
    "\n",
    "To read more about precision and recall, you can Search the Internet as usual, or visit https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's Investigate the misclassified image**\n",
    "\n",
    "Open up that folder and check the images. \n",
    "It seems that 58.png was classified wrongly. Why?\n",
    "\n",
    "58.png\n",
    "\n",
    "<img src=\"[Dataset] Module 21 images/test/58.png\" style=\"width:400px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "Recall our initial set of training images. <br />\n",
    "58.png looks much brighter than the training image for \"green\", which may suggest why it was mistaken as \"none\" (which was the \"brightest\" among the 4 training images)\n",
    "\n",
    "<img src=\"[Dataset] Module 21 images/cards.png\" style=\"float:left;\"/>\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us as humans, it is easy for us to tell that 58.png should be classified as green. \n",
    "\n",
    "However, remember that the feature we used to \"train\" the system was \"average color\" and we only supplied one training image. \n",
    "\n",
    "It seem that the average color of 58.png is closer to the average color of the background (background.png) rather than the training image (cardgreen_close.png). \n",
    "\n",
    "It will be left as an exercise for you to calculate the average color of the images respectively and uncover why it was misclassified. That will be your Challenge 1 later in this notebook.\n",
    "\n",
    "Meanwhile, can you think of a way to improve the model?\n",
    "\n",
    "### Open the folder of test images!\n",
    "You can open the folder of test images. Do they appear to be under different lighting conditions? We only trained our system using a single example for each colored card so far. Do you think having more training images might help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3 Training with more samples\n",
    " \n",
    "How about training it with more samples? <br />\n",
    "Recall what we did in section 1.1 to get trainX and trainY. If you have forgotten, do revisit section 1.1 to understand the code better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images for the label: red\n",
      "Loading training images for the label: green\n",
      "Loading training images for the label: black\n",
      "Loading training images for the label: none\n"
     ]
    }
   ],
   "source": [
    "trainX2 = []\n",
    "trainY2 = []\n",
    "import os\n",
    "\n",
    "# Let's loop through the training images in the 4 folders in the image subdirectory\n",
    "path = \"[Dataset] Module 21 images/\"\n",
    "for label in ('red','green','black','none'):\n",
    "    print (\"Loading training images for the label: \"+label)\n",
    "    \n",
    "    #Load all images inside the subfolder\n",
    "    for filename in os.listdir(path+label+\"/\"): \n",
    "        img = cv2.imread(path+label+\"/\"+filename)\n",
    "        img_features = averagecolor(img)\n",
    "        trainX2.append(img_features)\n",
    "        trainY2.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: How many images do we use to train our model now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print (len(trainX2))\n",
    "print (len(trainY2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Check with the subfolders!\n",
    "Open the red, green, black and none subfolders in the images directory on your computer. How many images are we loading in from each folder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After having loaded more training images, let us re-run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.png: black\n",
      "17.png: black\n",
      "18.png: black\n",
      "19.png: black\n",
      "20.png: black\n",
      "36.png: red\n",
      "37.png: red\n",
      "38.png: red\n",
      "39.png: red\n",
      "40.png: red\n",
      "56.png: green\n",
      "57.png: green\n",
      "58.png: green\n",
      "59.png: green\n",
      "60.png: green\n",
      "76.png: none\n",
      "77.png: none\n",
      "78.png: none\n",
      "79.png: none\n",
      "80.png: none\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       1.00      1.00      1.00         5\n",
      "       green       1.00      1.00      1.00         5\n",
      "        none       1.00      1.00      1.00         5\n",
      "         red       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n",
      "Correct :20. Wrong: 0. Correctly Classified: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"[Dataset] Module 21 images/test/\"\n",
    "filenames = []\n",
    "predictedY = []\n",
    "for filename in os.listdir(path):\n",
    "    img = cv2.imread(path+filename)\n",
    "    img_features = averagecolor(img)\n",
    "    calculated_distances = []\n",
    "    for card in (trainX2):\n",
    "        calculated_distances.append(np.linalg.norm(img_features-card))\n",
    "    prediction =  trainY2[np.argmin(calculated_distances)]\n",
    "    \n",
    "    print (filename + \": \" + prediction)\n",
    "    filenames.append(filename)\n",
    "    predictedY.append(prediction)\n",
    "\n",
    "# Evaluate Accuracy (the sklearn package provides a useful report)\n",
    "print ()\n",
    "print(classification_report(realtestY, predictedY))\n",
    "\n",
    "# Evaluate Accuracy\n",
    "print (evaluateaccuracy(filenames,predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What have we just done?**\n",
    "\n",
    "We have just seen how we \"trained\" the model for the kNN in section 1.1, and then used the model to predict which class the new card belonged to in section 1.2. We then went further in section 1.3 to explore how increasing the training data could help to improve the accuracy, eliminating the earlier error of misclassifying \"58.png\" as none when it was actually green.\n",
    "\n",
    "We used a very simplified example of the kNN algorithm which finds the k Nearest Neighbours to predict the class of the new image based on its closest neighbours. In the example above, the value of k was 1. Hence, we only searched for the nearest neighbour (the neighbour with the smallest calculated distance), and predicted the value of the test image based on the class of the nearest neighbour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a moment to reflect\n",
    "\n",
    "How does this method compare to the methods used in the previous session? \n",
    "\n",
    "Did you require more or less lines of code? Do you prefer defining the rules or letting the machine learn by itself? For most of you, you would probably find it easier to provide a set of training images than to have to define the rules manually. If you found it easier to define the rules and still had a rather robust system, what techniques did you use?\n",
    "\n",
    "How can we improve the system further? Would a mix of approaches do even better? Will this work with all types of images? Why or why not? Do write your notes in the Student Activity Guide.\n",
    "\n",
    "<br />\n",
    "<video controls src=\"[Dataset] Module 21 images/black_red_green.mp4\" style=\"width:400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic steps for building a classification model\n",
    "\n",
    "In section 1, we have quickly jumped into implementing a very simple classification model based on the kNN algorithm.\n",
    "In practice, training of computer vision models is typically done using frameworks like Keras, Tensorflow, Caffe, and MXNet, or libraries such as Scikit-Learn for Python. These frameworks and libraries contain various tools and make it easier to work with larger data sets and algorithms without having to code everything from scratch. \n",
    "\n",
    "Training can take hours, days or even weeks, often requiring machines with GPUs and more powerful compute capabilities. The model we built for kNN was a simplistic one using numpy arrays, for the sake of illustrating the concepts.\n",
    "\n",
    "Let us now explore the steps typically required for building a classification model (some of which were already done for you in this exercise):\n",
    "1. Gathering data\n",
    "1. Data Preparation (cleaning, labelling, etc.)\n",
    "1. Splitting the data into a training set and a test set\n",
    "1. Selecting an algorithm and training a model\n",
    "1. Evaluating the performance\n",
    "\n",
    "Selecting the algorithm to use was just one out of the 5 steps. For machine learning algorithms, the data preparation is very important. If you feed in wrong information, the model will naturally turn out wrong. The data needs to be representative and the features used needs to be relevant to your purpose. Otherwise, you may get very unreliable results.\n",
    "\n",
    "Similarly, any prior preprocessing and the features that you use for the model is important. Imagine trying to train a model that recognizes flowers of different colors but only using greyscale images (leaving out the important color features). In contrast, for optical character recognition (OCR), color may not be very useful and might not be included in the selected features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. That was kNN, how about Support Vector Machines?\n",
    "\n",
    "If you think about it, the k-Nearest-Neighbour algorithm did not really learn much, it basically stored the training data and did a lookup everytime an inference on a new image was required. \n",
    "\n",
    "In your math class, do you remember learning about deriving the equation of a line **y = mx + c?**\n",
    "\n",
    "What if we could also derive an equation or formula that could be used to predict the different classes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Support Vectors?\n",
    "\n",
    "Imagine you needed to classify O from X. Could you draw a single line that best separates all the X from the O?\n",
    "\n",
    "<img src=\"[Dataset] Module 21 images/svm1.jpg\" style=\"width: 300px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "Perhaps we could draw a line (blue line below). And this is a simple example of a Support Vector.  Anything to the left/top of the line could be classified as X, and anything to the right/bottom of the line could be classified as O. \n",
    "\n",
    "<img src=\"[Dataset] Module 21 images/svm2.jpg\" style=\"width: 300px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The math behind SVM will be outside the scope of this session, but you are encouraged to read more. https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/ (In the link, it illustrates with diagrams how a single linear vector can separate 2 distinct classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go on to explore how Support Vector Machines (SVM) work in practice, making use of the python scikit-learn library. First, \"derive the equation\" of the Support Vector, then \"use the equation\" to run the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since SVM uses numerical values, we first encode our labels into numerical\n",
    "from sklearn.preprocessing import LabelEncoder  #encode labels into numerical\n",
    "encoder = LabelEncoder()                        #encode labels into numerical\n",
    "encodedtrainY2 = encoder.fit_transform(trainY2) #encode labels into numerical\n",
    "\n",
    "from sklearn import svm\n",
    "model = svm.SVC(gamma=\"scale\", decision_function_shape='ovr')\n",
    "model.fit(trainX2, encodedtrainY2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does LabelEncoder do? Let's look at the function result. \n",
    "You can read more about LabelEncoder [here](https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print (encodedtrainY2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-depth understanding of SVM is beyond the scope of this moodule, but feel free to learn more [here](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have obtained our SVM model.  \n",
    "\n",
    "### Let's run the predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.png: black\n",
      "17.png: black\n",
      "18.png: black\n",
      "19.png: black\n",
      "20.png: black\n",
      "36.png: red\n",
      "37.png: red\n",
      "38.png: red\n",
      "39.png: red\n",
      "40.png: red\n",
      "56.png: green\n",
      "57.png: green\n",
      "58.png: green\n",
      "59.png: green\n",
      "60.png: green\n",
      "76.png: none\n",
      "77.png: none\n",
      "78.png: none\n",
      "79.png: none\n",
      "80.png: none\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       1.00      1.00      1.00         5\n",
      "       green       1.00      1.00      1.00         5\n",
      "        none       1.00      1.00      1.00         5\n",
      "         red       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n",
      "Correct :20. Wrong: 0. Correctly Classified: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = \"[Dataset] Module 21 images/test/\"\n",
    "filenames = []\n",
    "predictedY = []\n",
    "for filename in os.listdir(path):\n",
    "    img = cv2.imread(path+filename)\n",
    "    img_features = averagecolor(img)\n",
    "    prediction = model.predict([img_features])[0]\n",
    "    \n",
    "    #decode the prediction\n",
    "    prediction = encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    print (filename + \": \" + prediction)\n",
    "    filenames.append(filename)\n",
    "    predictedY.append(prediction)\n",
    "\n",
    "# Evaluate Accuracy (the sklearn package provides a useful report)\n",
    "print ()\n",
    "print(classification_report(realtestY, predictedY))\n",
    "\n",
    "# Evaluate Accuracy\n",
    "print (evaluateaccuracy(filenames,predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which one is more accurate?\n",
    "\n",
    "Do you think SVM is more effective at getting more correct classifications than kNN or vice versa? \n",
    "\n",
    "It depends on the problem. And for SVM, there are also other parameters that will need to be tuned that are outside the scope of this session. These parameters will guide the model generation process. For example, the model needs to know what kind of Support Vector to generate. A \"straight line\" might work for some datasets, but for others, we might need a curve or more complex support vectors.\n",
    "\n",
    "For illustration, imagine trying to fit a straight line to classify the Os and Xs below. Perhaps you might need an equation for a circle instead.\n",
    "\n",
    "<img src=\"[Dataset] Module 21 images/svm3.jpg\" style=\"width:400px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "You can refer to the links at the end of this section if you wish to find out more.\n",
    "\n",
    "Up to this point, trained out model using trainX2 and trainY2, then tested our model against a separate set of images and it seemed to perform well. However, working well on a small test set does not mean that it will always work well. Let us test again on another image that has not been tested before. The human eye can easily tell which color it is. But will the model that seems to be working perfectly so far be able to classify it correctly?\n",
    "\n",
    "<img src=\"[Dataset] Module 21 images/cardtestagain.png\" style=\"width:400px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n"
     ]
    }
   ],
   "source": [
    "imagenew = cv2.imread(\"[Dataset] Module 21 images/cardtestagain.png\")\n",
    "imagenew_features = averagecolor(imagenew)\n",
    "prediction = (model.predict([imagenew_features])[0])\n",
    "\n",
    "#decode the prediction from numerical to labels\n",
    "print(encoder.inverse_transform([prediction])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What went wrong?\n",
    "\n",
    "Unfortunately, the image appears to be wrongly classified as none instead of red. <br />\n",
    "It would be hard to dig into why the SVM model classified wrongly in this instance without digging deep into the math which is outside the scope of this session. A simple analogy would be that it might be difficult to try to fit a curve into the equation for a straight line. Just like how y=mx+c would be the wrong equation to use for a curve.\n",
    "\n",
    "**Side Tip:** When designing solutions using machine learning, aim to train the most accurate model but do also take some time to plan for contingencies when the model may not give the correct result. Also consider what could be the impacts of wrong results on your application, and take steps to mitigate the risks. For example, if it is piece of machinary being guided by computer vision, are there other sensors that can also be used to trigger an emergency stop before it crashes into something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, What does our kNN algorithm think about the same image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "calculated_distances = []\n",
    "for card in (trainX):\n",
    "    calculated_distances.append(np.linalg.norm(imagenew_features-card))\n",
    "print(trainY2[np.argmin(calculated_distances)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does that mean kNN is always more reliable?\n",
    "\n",
    "Let's try one more image:\n",
    "\n",
    "<img src=\"[Dataset] Module 21 images/cardtestagain2.png\" style=\"width:400px; float:left;\" />\n",
    "<div style=\"clear:both;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: none\n",
      "kNN: none\n"
     ]
    }
   ],
   "source": [
    "imagenew = cv2.imread(\"[Dataset] Module 21 images/cardtestagain2.png\")\n",
    "imagenew_features = averagecolor(imagenew)\n",
    "calculated_distances = []\n",
    "for card in (trainX2):\n",
    "    calculated_distances.append(np.linalg.norm(imagenew_features-card))\n",
    "    \n",
    "print(\"SVM: \"+str(encoder.inverse_transform([ model.predict([imagenew_features])[0] ])[0]))\n",
    "print(\"kNN: \"+str(trainY2[np.argmin(calculated_distances)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image above, can you guess why kNN wrongly classified the algorithm as none instead of green? \n",
    "\n",
    "You can calculate the average color of the image to find out why.\n",
    "\n",
    "And yes, you can train the model with more images to mitigate these issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: The math behind SVM will be outside the scope of this session, but you are encouraged to read more. https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/ _ (In the link, it illustrates with diagrams how a single linear vector can separate 2 distinct classes)\n",
    "\n",
    "In our experiment, however, we used it to separate more than 2 classes. You can learn more about the multi-class classification using SVM and view code samples using the documentation at https://scikit-learn.org/stable/modules/svm.html#multi-class-classification And do remember to Search the Internet if you need more help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! \n",
    "\n",
    "## It's now time for you to do some hands on! \n",
    "\n",
    "You have completed a very quick introduction to machine learning, and seen the progression from a rule based approach to a machine learning approach. You have also used training data to train the kNN and SVM models for classification, and seen the results as well as some limitations. There is certainly a lot more to be learnt, but you can already start building!\n",
    "\n",
    "Anytime you need some help, you can always search your friendly Internet. Here are some quicklinks to help: <br />\n",
    "- https://docs.opencv.org/4.0.0/d2/d96/tutorial_py_table_of_contents_imgproc.html\n",
    "- https://scikit-learn.org/stable/documentation.html\n",
    "\n",
    "**Tip:** Remember the basic computer vision techniques that you picked up at the previous session. You can make use of them as you think of what features would be useful for feeding into your model. Color spaces, Thresholding, Contour Detection, Geometric Transformations, direct manipulation of image numpy arrays and more. Sometimes, basic methods can be the most effective for the task at hand.\n",
    "\n",
    "Whatever you build, keep in mind your purpose and objective and think of different possible approaches. Also keep in mind the possible impacts when a machine learning algorithm makes a misclassification and plan for ways to mitigate the risks. For example, if you know that your model identifies red cards very well but sometimes mixes up green and blue cards, then you might want to design a solution using red cards instead of green and blue. And you might add in other layers of check such as, if a green/black card is detected, require a security personnel to perform a second layer check.\n",
    "\n",
    "As with other scenarios where there may be a probability of error, explore complementing the design of your real-world solutions with other techniques or hardware sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: What is the average color of \"images/test/58.png\"?\n",
    "\n",
    "Remember that it was initially classified incorrectly? Let us explore why.\n",
    "Store the result in a variable \"cha1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208.77603841 223.6275293  120.75616536]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_58 = cv2.imread(\"[Dataset] Module 21 images/test/58.png\")\n",
    "cha1 =averagecolor(image_58)\n",
    "print (cha1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: What is the average color of \"images/background.png\"?\n",
    "\n",
    "Store the result in a variable \"cha2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247.9326888  241.13666016 241.89832357]\n"
     ]
    }
   ],
   "source": [
    "image_bg = cv2.imread(\"[Dataset] Module 21 images/background.png\")\n",
    "cha2 =averagecolor(image_bg)\n",
    "print (cha2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: What is the average color of \"images/cardgreen_close.png\"?\n",
    "\n",
    "Store the result in a variable \"cha3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119.53976563 133.40338216  61.1089388 ]\n"
     ]
    }
   ],
   "source": [
    "image_green = cv2.imread(\"[Dataset] Module 21 images/cardgreen_close.png\")\n",
    "cha3 =averagecolor(image_green)\n",
    "print (cha3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: 58.png vs Background. Calculate the distance between cha1 and cha2\n",
    "\n",
    "Recall how the euclidean distance was calculated in section 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.51161592391296"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(cha2-cha1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5: 58.png vs Green. Calculate the distance between cha1 and cha3\n",
    "\n",
    "Recall how the euclidean distance was calculated in section 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140.2187603130577"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(cha3-cha1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the distance in Challenge 4 (58.png vs. background) or the distance in Challenge 5 (58.png vs. green) smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smaller distance implies a greater similarity. Hence, 58.png was classified as more similar to the background, than to the green card based on the 4 \"training images\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: Modify the function averagecolor to convert the image to greyscale before extracting the features\n",
    "\n",
    "Recall how you converted an image to greyscale in the previous session. Calculate the accuracy of the kNN model when we use average greyscale instead of averagecolor. (i.e. Run section 1.3 using your updated averagecolor function. Rerun both the training and inference steps) What is the new accuracy? Does it perform better? Why do you think so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.png: black\n",
      "17.png: black\n",
      "18.png: black\n",
      "19.png: black\n",
      "20.png: black\n",
      "36.png: black\n",
      "37.png: black\n",
      "38.png: black\n",
      "39.png: black\n",
      "40.png: black\n",
      "56.png: green\n",
      "57.png: green\n",
      "58.png: none\n",
      "59.png: green\n",
      "60.png: black\n",
      "76.png: none\n",
      "77.png: none\n",
      "78.png: none\n",
      "79.png: none\n",
      "80.png: none\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       0.45      1.00      0.62         5\n",
      "       green       1.00      0.60      0.75         5\n",
      "        none       0.83      1.00      0.91         5\n",
      "         red       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.57      0.65      0.57        20\n",
      "weighted avg       0.57      0.65      0.57        20\n",
      "\n",
      "\n",
      "Wrong Predictions: (filename, labelled, predicted) \n",
      "[['36.png' 'red' 'black']\n",
      " ['37.png' 'red' 'black']\n",
      " ['38.png' 'red' 'black']\n",
      " ['39.png' 'red' 'black']\n",
      " ['40.png' 'red' 'black']\n",
      " ['58.png' 'green' 'none']\n",
      " ['60.png' 'green' 'black']]\n",
      "Correct :13. Wrong: 7. Correctly Classified: 65.0%\n"
     ]
    }
   ],
   "source": [
    "# You can modify the function below\n",
    "def averagecolor_grey(image):\n",
    "    grey = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.mean(grey, axis=(0, 1))\n",
    "       \n",
    "import os\n",
    "path = \"[Dataset] Module 21 images/test/\"\n",
    "predictedY = []\n",
    "filenames = []\n",
    "for filename in os.listdir(path):\n",
    "    img = cv2.imread(path+filename)\n",
    "    img_features = averagecolor_grey(img)\n",
    "    calculated_distances = []\n",
    "    for card in (trainX):\n",
    "        calculated_distances.append(np.linalg.norm(img_features-card))\n",
    "    prediction = trainY[np.argmin(calculated_distances)]\n",
    "    \n",
    "    print (filename + \": \" + prediction) #Print out the inferences\n",
    "    filenames.append(filename)\n",
    "    predictedY.append(prediction)\n",
    "\n",
    "# Evaluate Accuracy (the sklearn package provides a useful report)\n",
    "print ()\n",
    "print(classification_report(realtestY, predictedY))\n",
    "\n",
    "# Evaluate Accuracy (our own custom method to output the filenames of the misclassified entries)\n",
    "print ()\n",
    "print (evaluateaccuracy(filenames,predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Topic: Beyond classification, can we localize where the colored card is within the image using what we have learnt?\n",
    "\n",
    "Since we used average color as the feature for our model, could we also use it to classify each individual pixel within the image, rather than just classifying the entire image?\n",
    "\n",
    "Knowing which pixels belong to the \"color class\" could help us to understand where in the image our card is. This is an example of _localization_. \n",
    "\n",
    "Let's use the 3 sample images to do this experiment. images/cardblack.png, images/cardred.png, images/cardgreen.png\n",
    "\n",
    "For a start, can we identify where the red card is in the picture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;KNeighborsClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier(n_neighbors=1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's a simpler way to use the kNN classifier from the sklearn package. \n",
    "# We don't need to write it from scratch now that we know how it works\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnmodel = KNeighborsClassifier(n_neighbors=1)\n",
    "knnmodel.fit(trainX2, trainY2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_localization = cv2.imread(\"[Dataset] Module 21 images/cardred.png\")\n",
    "\n",
    "# First we will need to arrange the pixels into a format that our model accepts (a 2D array)\n",
    "# We can use the numpy array methods to help us do that easily\n",
    "temp = card_localization.reshape((307200,3)) \n",
    "\n",
    "#predict the color of each pixel\n",
    "prediction = knnmodel.predict(temp)             \n",
    "\n",
    "# We can arrange the predictions back into the shape of the image that we are familiar with\n",
    "masklabels = (prediction.reshape((480,640)))\n",
    "\n",
    "#create a mask for the class we are interested in! (Recall what are masks in the previous session)\n",
    "canvas = np.zeros(card_localization.shape[:2],dtype=\"uint8\")\n",
    "canvas[masklabels==\"red\"]=255\n",
    "\n",
    "#get the largest blob and find the location\n",
    "(cnts,_) = cv2.findContours(canvas, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = sorted(cnts, key=lambda cnts: cv2.boundingRect(cnts)[1])[:1]  #sort contours from top to bottom.\n",
    "for (i, c) in enumerate(cnts):    \n",
    "    (x, y, w, h) = cv2.boundingRect(c)                  \n",
    "    cv2.rectangle(card_localization, (x,y), (x+w,y+h), (0,255,0),2)     # Draw the bounding boxes in red\n",
    "\n",
    "\n",
    "#Display the results\n",
    "cv2.imshow(\"Localization\",canvas)\n",
    "cv2.imshow(\"Marked Card\",card_localization) #see the bounding box drawn here\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same concept, we could also perform _image segmentation_ which is basically assigning a class label to each pixel. In the example below, you can visualize the 4 labels \"red\", \"green\", \"black\" and \"none\" (represented by white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_localization = cv2.imread(\"[Dataset] Module 21 images/cardred.png\")\n",
    "\n",
    "# First we will need to arrange the pixels into a format that our model accepts (a 2D array)\n",
    "# We can use the numpy array methods to help us do that easily\n",
    "temp = card_localization.reshape((307200,3)) \n",
    "\n",
    "#predict the color of each pixel\n",
    "prediction = knnmodel.predict(temp)             \n",
    "\n",
    "# We can arrange the predictions back into the shape of the image that we are familiar with\n",
    "masklabels = (prediction.reshape((480,640)))\n",
    "\n",
    "#create a mask for the class we are interested in! (Recall what are masks in the previous session)\n",
    "canvas = np.zeros(card_localization.shape,dtype=\"uint8\")\n",
    "canvas[masklabels==\"green\"]=(0,255,0)\n",
    "canvas[masklabels==\"red\"]=(0,0,255)\n",
    "canvas[masklabels==\"black\"]=(0,0,0)\n",
    "canvas[masklabels==\"none\"]=(255,255,255)\n",
    "\n",
    "#Display the results\n",
    "cv2.imshow(\"Image Segmentation\",canvas)\n",
    "cv2.imshow(\"Original Card\",card_localization)\n",
    "#cv2.imshow(\"Masked Image\",cv2.bitwise_and(card_localization,card_localization,mask=canvas))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image segmentation is used for applications such as medical imaging and video surveillance. This is because they not only need to know what is in the image, but also to identify parts of the image that may be more important than others. For example, marking a possible tumour in the medical scan, or a suspicious person in the CCTV. \n",
    "\n",
    "Notice that our model does have some imperfections, and how some parts of the fingers are wrongly inferred as black or red. Nonetheless, this example serves to illustrate the concept of image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 7: Which distance calculation metric does the sklearn kNN model use by default? Is it also the euclidean distance that we used?\n",
    "\n",
    "You can look up https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric : string or callable, default ‘minkowski’\n",
    "# the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \n",
    "# See the documentation of the DistanceMetric class for a list of available metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 8: Using the kNN algorithm, mask out just the red card from \"images/cardmixed.png\", and calculate the fraction of red pixels vs. total number of pixels in that image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_localization = cv2.imread(\"[Dataset] Module 21 images/cardmixed.png\")\n",
    "\n",
    "# First we will need to arrange the pixels into a format that our model accepts (a 2D array)\n",
    "# We can use the numpy array methods to help us do that easily\n",
    "temp = card_localization.reshape((307200,3)) \n",
    "\n",
    "#predict the color of each pixel\n",
    "prediction = knnmodel.predict(temp)             \n",
    "\n",
    "# We can arrange the predictions back into the shape of the image that we are familiar with\n",
    "masklabels = (prediction.reshape((480,640)))\n",
    "\n",
    "#create a mask for the class we are interested in! (Recall what are masks in the previous session)\n",
    "canvas = np.zeros(card_localization.shape[:2],dtype=\"uint8\")\n",
    "canvas[masklabels==\"red\"]=255\n",
    "\n",
    "#get the largest blob and find the location\n",
    "(cnts,_) = cv2.findContours(canvas, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = sorted(cnts, key=lambda cnts: cv2.boundingRect(cnts)[1])[:1]  #sort contours from top to bottom.\n",
    "for (i, c) in enumerate(cnts):    \n",
    "    (x, y, w, h) = cv2.boundingRect(c)                  \n",
    "    cv2.rectangle(card_localization, (x,y), (x+w,y+h), (0,255,0),2)     # Draw the bounding boxes in red\n",
    "\n",
    "\n",
    "#Display the results\n",
    "cv2.imshow(\"Localization\",canvas)\n",
    "cv2.imshow(\"Marked Card\",card_localization) #see the bounding box drawn here\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15045247395833333\n"
     ]
    }
   ],
   "source": [
    "#find fraction\n",
    "red_total_fraction = np.count_nonzero (canvas)/(480*640)\n",
    "print(red_total_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In this case, we happened to be able to use the model to predict the class of each pixel because our model was built using color as a feature. However this method may not always be applicable depending on the features used for different models. You can search the Internet to explore other methods of localization. \n",
    "\n",
    "Artificial Neural Networks (ANN), specifically Convolutional Neural Networks (CNN) have recently become very popular due to their much higher level of effectiveness. \n",
    "\n",
    "Have you heard of the DJI Tello drones? They make use of computer vision to improve the flight quality too, and they are also powered by an Intel MYRIAD VPU!\n",
    "https://www.movidius.com/news/hello-tello-ryze-announces-intel-myriad-vpu-powered-toy-drone\n",
    "\n",
    "If you remember, earlier in this lesson we also discussed about complementing machine vision with other sensors. What other sensors do you think are onboard the Tello drone apart from the camera?\n",
    "https://www.ryzerobotics.com/tello/specs\n",
    "\n",
    "What do you think the range finder and the barometer are used to sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
